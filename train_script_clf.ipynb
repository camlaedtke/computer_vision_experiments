{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay, PiecewiseConstantDecay\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from models.clf.hrnet_clf_accumilate import HRNet_CLF\n",
    "\n",
    "K.clear_session()\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "def enable_amp():\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(physical_devices,\"\\n\")\n",
    "enable_amp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetLoader():\n",
    "    \n",
    "    def __init__(self, img_height, img_width, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.MEAN = np.array([0.485, 0.456, 0.406])\n",
    "        self.STD = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def random_crop(self, image):\n",
    "\n",
    "        scales = tf.convert_to_tensor(np.array([0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0]))\n",
    "        scale = scales[tf.random.uniform(shape=[], minval=0, maxval=8, dtype=tf.int32)]\n",
    "        scale = tf.cast(scale, tf.float32)\n",
    "\n",
    "        shape = tf.cast(tf.shape(image), tf.float32)\n",
    "        h = tf.cast(shape[0] * scale, tf.int32)\n",
    "        w = tf.cast(shape[1] * scale, tf.int32)\n",
    "        image = tf.image.random_crop(image, size=[h, w, 3])\n",
    "        return image\n",
    "\n",
    "    @tf.function\n",
    "    def normalize(self, image):\n",
    "        image = image / 255.0\n",
    "        image = image - self.MEAN\n",
    "        image = image / self.STD\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def load_image_train(self, datapoint):\n",
    "\n",
    "        img = datapoint['image']\n",
    "        label = datapoint['label']\n",
    "        label = tf.one_hot(tf.cast(label, tf.int32), self.n_classes)\n",
    "\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.flip_left_right(img)\n",
    "\n",
    "        img = self.random_crop(img)\n",
    "        img = tf.image.resize(img, (self.img_height, self.img_width))\n",
    "        img = self.normalize(tf.cast(img, tf.float32))\n",
    "\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            img = tf.image.random_brightness(img, 0.05)\n",
    "            img = tf.image.random_saturation(img, 0.6, 1.6)\n",
    "            img = tf.image.random_contrast(img, 0.7, 1.3)\n",
    "            img = tf.image.random_hue(img, 0.05)\n",
    "\n",
    "        return img, label\n",
    "   \n",
    "\n",
    "    def load_image_test(self, datapoint):\n",
    "        img = datapoint['image']\n",
    "        label = datapoint['label']\n",
    "        label = tf.one_hot(tf.cast(label, tf.int32), self.n_classes)\n",
    "        img = tf.image.resize(img, (self.img_height, self.img_width))\n",
    "        img = self.normalize(tf.cast(img, tf.float32))\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_height = 224\n",
    "img_width = 224\n",
    "n_classes = 1000\n",
    "\n",
    "pipeline = ImageNetLoader(\n",
    "    n_classes = n_classes,\n",
    "    img_height = img_height,\n",
    "    img_width = img_width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imagenet2012:5.0.0', data_dir='/workspace/tensorflow_datasets/', \n",
    "                          with_info=True, shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train'].map(pipeline.load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid = dataset['validation'].map(pipeline.load_image_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "VALID_LENGTH = info.splits['validation'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "ACCUM_STEPS = 2\n",
    "BUFFER_SIZE = 8192\n",
    "ADJ_BATCH_SIZE = BATCH_SIZE * ACCUM_STEPS\n",
    "print(\"Effective batch size: {}\".format(ADJ_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list, title=True):\n",
    "    plt.figure(figsize=(15, 5)) # dpi=200\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for image, label in train.take(4): \n",
    "    sample_image, sample_label = image, label\n",
    "\n",
    "print(sample_image.shape, sample_label.shape)\n",
    "display([sample_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = HRNet_CLF(\n",
    "    stage1_cfg = {'NUM_MODULES': 1,'NUM_BRANCHES': 1,'BLOCK': 'BOTTLENECK','NUM_BLOCKS': [4]}, \n",
    "    stage2_cfg = {'NUM_MODULES': 1,'NUM_BRANCHES': 2,'BLOCK': 'BASIC',     'NUM_BLOCKS': [4, 4]},\n",
    "    stage3_cfg = {'NUM_MODULES': 4,'NUM_BRANCHES': 3,'BLOCK': 'BASIC',     'NUM_BLOCKS': [4, 4, 4]},\n",
    "    stage4_cfg = {'NUM_MODULES': 3,'NUM_BRANCHES': 4,'BLOCK': 'BASIC',     'NUM_BLOCKS': [4, 4, 4, 4]},\n",
    "    input_height = img_height, \n",
    "    input_width = img_width, \n",
    "    n_classes = n_classes, \n",
    "    W = 48,\n",
    "    # GN_GROUPS=24,\n",
    "    ACCUM_STEPS=ACCUM_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Model: \"HRNet_W48\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d (Conv2D)              multiple                  1728      \n",
    "_________________________________________________________________\n",
    "batch_normalization (BatchNo multiple                  256       \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            multiple                  36864     \n",
    "_________________________________________________________________\n",
    "batch_normalization_1 (Batch multiple                  256       \n",
    "_________________________________________________________________\n",
    "re_lu (ReLU)                 multiple                  0         \n",
    "_________________________________________________________________\n",
    "sequential_1 (Sequential)    (1, 128, 256, 192)        168192    \n",
    "_________________________________________________________________\n",
    "sequential_2 (Sequential)    (1, 128, 256, 48)         83136     \n",
    "_________________________________________________________________\n",
    "sequential_4 (Sequential)    (1, 64, 128, 96)          166272    \n",
    "_________________________________________________________________\n",
    "high_resolution_module (High multiple                  880704    \n",
    "_________________________________________________________________\n",
    "sequential_11 (Sequential)   (1, 32, 64, 192)          166656    \n",
    "_________________________________________________________________\n",
    "high_resolution_module_1 (Hi multiple                  3840576   \n",
    "_________________________________________________________________\n",
    "high_resolution_module_2 (Hi multiple                  3840576   \n",
    "_________________________________________________________________\n",
    "high_resolution_module_3 (Hi multiple                  3840576   \n",
    "_________________________________________________________________\n",
    "high_resolution_module_4 (Hi multiple                  3840576   \n",
    "_________________________________________________________________\n",
    "sequential_65 (Sequential)   (1, 16, 32, 384)          665088    \n",
    "_________________________________________________________________\n",
    "high_resolution_module_5 (Hi multiple                  15891072  \n",
    "_________________________________________________________________\n",
    "high_resolution_module_6 (Hi multiple                  15891072  \n",
    "_________________________________________________________________\n",
    "high_resolution_module_7 (Hi multiple                  15891072  \n",
    "_________________________________________________________________\n",
    "sequential_144 (Sequential)  (1, 128, 256, 20)         535700    \n",
    "=================================================================\n",
    "Total params: 65,740,372\n",
    "Trainable params: 65,655,732\n",
    "Non-trainable params: 84,640\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"weights/\"+model.name+\".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "VALIDATION_STEPS = VALID_LENGTH // BATCH_SIZE\n",
    "DECAY_STEPS = (STEPS_PER_EPOCH * EPOCHS) // ACCUM_STEPS\n",
    "DECAY_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_EPOCH = 14\n",
    "E1 = 30 - CURR_EPOCH\n",
    "E2 = 60 - CURR_EPOCH\n",
    "E3 = 90 - CURR_EPOCH\n",
    "\n",
    "S1 = (STEPS_PER_EPOCH * E1) // ACCUM_STEPS\n",
    "S2 = (STEPS_PER_EPOCH * E2) // ACCUM_STEPS\n",
    "S3 = (STEPS_PER_EPOCH * E3) // ACCUM_STEPS\n",
    "\n",
    "print(\"--- LR decay --- \\nstep {}: {} \\nstep {}: {} \\nstep {}: {}\".format(S1, 1e-2, S2, 1e-3, S3, 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = PiecewiseConstantDecay(boundaries = [S1, S2, S3], values = [0.1, 0.01, 0.001, 0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = SGD(learning_rate=learning_rate_fn, momentum=0.9, decay=0.0001),\n",
    "    loss=CategoricalCrossentropy(from_logits=True),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(MODEL_PATH, monitor='val_accuracy', mode='max', \n",
    "                    verbose=2, save_best_only=True, save_weights_only=True)    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
